the largest part of compute is the FFN
mixtral:
That’s ~67 million parameters per attention layer, versus ~118 million per feedforward block (2 × 4096 × 14336) you estimated earlier.